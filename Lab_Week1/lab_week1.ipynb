{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqRSnO68lKJR"
      },
      "source": [
        "# COMP3132 - Lab Week 1\n",
        "\n",
        "# Building an LLM-Powered Chatbot: A Hands-On Guide in Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Name: Drasti Parikh\n",
        "# Student Id: 101419828"
      ],
      "metadata": {
        "id": "jO5y_vLvOexA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILpJ2QAOWH0N"
      },
      "source": [
        "## Google Colab Configuration\n",
        "\n",
        "### Some of our labs this semester will be painfully slow if without a GPU. The easies way to get access to a GPU accelerated Jupyter notebook is to enable the `T4 GPU runtime` on Google Colab:\n",
        "\n",
        "### 1. Navigate to `Runtime`.\n",
        "### 2. Select `Change runtime type`.\n",
        "### 3. Choose `Hardware accelerator`.\n",
        "### 4. Select `T4 GPU`.\n",
        "\n",
        "### **Note:** This notebook can be run on `CPU` without any noticeable difference in performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wxpgrcGYZJHG"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image, display"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "0Gedb1EAT3Pd"
      },
      "outputs": [],
      "source": [
        "!pip install python-dotenv\n",
        "!pip install jupyter_bokeh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVFndvYeTku2"
      },
      "source": [
        "# Online Chatbot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIizeeRWTku4"
      },
      "source": [
        "### Go to https://api.together.ai/playground/chat/meta-llama/Llama-2-7b-chat-hf to chat with the model online on `togerther.ai` website and play with the chatbot by changing the configurations and hyper-parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ylsoOI0SRYO"
      },
      "source": [
        "# A Brief Theory\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-cwOAjzWH0P"
      },
      "source": [
        "## Training a Language Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tXq3n80DWH0P"
      },
      "outputs": [],
      "source": [
        "image_path = './assets/LLM_train.png'\n",
        "display(Image(image_path, width=600))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SxrSLigOlTkh"
      },
      "source": [
        "## Base Vs. Chat Models\n",
        "\n",
        "### After training the LLMs with this paradigm on a very large amount of data (such as the entire internet), we will have a model, also known as a `foundation` model or `base` model, that can predict the next word repeatedly to form a sentence.\n",
        "\n",
        "### To enable the model to engage in conversations, we further fine-tune the base model using instructions, such as question-answer pairs. These models are referred to as `instruction-tuned` or `chat` models.\n",
        "\n",
        "### You can observe the different behaviors of the base and instruction-tuned models in the following slide."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CD0rO0IEWZMh"
      },
      "outputs": [],
      "source": [
        "image_path = './assets/baseVSinstruct.png'\n",
        "display(Image(image_path, width=800))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhQZl868Tku4"
      },
      "source": [
        "## Interacting with Model Programmatically"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x1CnpZpbTku5"
      },
      "outputs": [],
      "source": [
        "image_path = './assets/modelaccess.png'\n",
        "display(Image(image_path, width=500))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wh1tK_PPlF9Y"
      },
      "source": [
        "# Designing Our Own Chatbot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0zXImou1Tku5"
      },
      "source": [
        "## API Call to the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npNiuaiEYA2o"
      },
      "source": [
        "### Getting API KEY\n",
        "\n",
        "#### - Go to https://api.together.xyz/settings/api-keys to get your API key."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86cIAvU0YKNY"
      },
      "source": [
        "#### Importing the API Key to Colab\n",
        "\n",
        "1. On the left-side vertical menu, select the `key` icon.\n",
        "2. Add a secret key with the following details:\n",
        "   - **Name**: `TOGETHER_API_KEY`\n",
        "   - **Value**: `<your API key>`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0v-Y7RV0X_NE"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "api_key = userdata.get('TOGETHER_API_KEY')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1J4cqDmTku6"
      },
      "source": [
        "### Function to call the API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NBz09yEUTku7"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "# from dotenv import load_dotenv, find_dotenv\n",
        "import warnings\n",
        "import requests\n",
        "import json\n",
        "import time\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "url = \"https://api.together.xyz/inference\"\n",
        "\n",
        "headers = {\n",
        "        \"Authorization\": f\"Bearer {api_key}\",\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "\n",
        "\n",
        "import time\n",
        "def llama(prompt,\n",
        "          add_inst=True,\n",
        "          model=\"meta-llama/Llama-2-7b-chat-hf\",\n",
        "          temperature=0.0,\n",
        "          max_tokens=1024,\n",
        "          verbose=False,\n",
        "          url=url,\n",
        "          headers=headers,\n",
        "          base = 2, # number of seconds to wait\n",
        "          max_tries=3):\n",
        "\n",
        "    if add_inst:\n",
        "        prompt = f\"[INST]{prompt}[/INST]\"\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"Prompt:\\n{prompt}\\n\")\n",
        "        print(f\"model: {model}\")\n",
        "\n",
        "    data = {\n",
        "            \"model\": model,\n",
        "            \"prompt\": prompt,\n",
        "            \"temperature\": temperature,\n",
        "            \"max_tokens\": max_tokens\n",
        "        }\n",
        "\n",
        "    # Allow multiple attempts to call the API incase of downtime.\n",
        "    # Return provided response to user after 3 failed attempts.\n",
        "    wait_seconds = [base**i for i in range(max_tries)]\n",
        "\n",
        "    for num_tries in range(max_tries):\n",
        "        try:\n",
        "            response = requests.post(url, headers=headers, json=data)\n",
        "            return response.json()['output']['choices'][0]['text']\n",
        "        except Exception as e:\n",
        "            if response.status_code != 500:\n",
        "                return response.json()\n",
        "\n",
        "            print(f\"error message: {e}\")\n",
        "            print(f\"response object: {response}\")\n",
        "            print(f\"num_tries {num_tries}\")\n",
        "            print(f\"Waiting {wait_seconds[num_tries]} seconds before automatically trying again.\")\n",
        "            time.sleep(wait_seconds[num_tries])\n",
        "\n",
        "    print(f\"Tried {max_tries} times to make API call to get a valid response object\")\n",
        "    print(\"Returning provided response\")\n",
        "    return response\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pl752Qo5Lpmd"
      },
      "source": [
        "### **Note:** Default model is `\"meta-llama/Llama-2-7b-chat-hf\"` but can you can change it by finding the model name from https://api.together.ai/playground/chat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0UVk9rmTku8"
      },
      "source": [
        "## General testing the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "teSmKBj-Tku8"
      },
      "outputs": [],
      "source": [
        "# pass prompt to the llama function, store output as 'response' then print\n",
        "prompt = \"Tell me a joke about software developers.\"\n",
        "response = llama(prompt, temperature=0.0)  # temperature is a hyperparameter that controls randomness in the response\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GtIx3E0PTku9"
      },
      "outputs": [],
      "source": [
        "prompt = \"What is the capital of France?\"\n",
        "response = llama(prompt, verbose=True) # verbose=True will print the prompt\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7DSSINATku9"
      },
      "source": [
        "## Exercise 1: General testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFSl58nmTku9"
      },
      "source": [
        "#### 1. Change the `temprarature` parameter from 0.0 to 0.9 and see the difference in the responses.\n",
        "#### Note: temperature parameter is a number between 0.0 and 1.0. It controls the randomness of the responses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8PwICCgsTku9"
      },
      "outputs": [],
      "source": [
        "prompt = \"Tell me a joke about software developers.\"\n",
        "response = llama(prompt, temperature=0.9)\n",
        "print(response)\n",
        "\n",
        "prompt = \"What is the capital of France?\"\n",
        "response = llama(prompt, verbose=True)\n",
        "print(response)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXem8QPfTku9"
      },
      "source": [
        "### 2. what does `verbose` argument do?\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHt0rq5QTku9"
      },
      "source": [
        "#### your answer here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ecA__NVTku9"
      },
      "source": [
        "## Role prompting\n",
        "\n",
        "#### - Roles give context to LLMs what type of answers are desired.\n",
        "#### - LLMs often gives more consistent responses when provided with a role.\n",
        "#### - First, try standard prompt and see the response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G43Y1R4HTku9"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"\n",
        "How can I answer this question from my friend:\n",
        "What is the meaning of life?\n",
        "\"\"\"\n",
        "response = llama(prompt)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "461pt7uDTku-"
      },
      "source": [
        "###  Now, try it by giving the model a `role`, and within the role, a `tone` using which it should respond with."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OBlxI0qyTku-"
      },
      "outputs": [],
      "source": [
        "role = \"\"\"\n",
        "Your role is a life coach \\\n",
        "who gives advice to people about living a good life.\\\n",
        "You attempt to provide unbiased advice.\n",
        "You respond in the tone of an Amitabh Bachchan.\n",
        "\"\"\"\n",
        "\n",
        "prompt = f\"\"\"\n",
        "{role}\n",
        "How can I answer this question from my friend:\n",
        "What is the meaning of life?\n",
        "\"\"\"\n",
        "response = llama(prompt)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ndaDyN-Tku-"
      },
      "source": [
        "## Excercise 2: Role prompting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KuFXfjX2Tku-"
      },
      "source": [
        "#### Role: Beginner python tutor\n",
        "#### Task: Explain how to create a list and add an element to it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ZysdT1qTku-"
      },
      "outputs": [],
      "source": [
        "role = \"\"\"\n",
        "Your role is a life coach \\\n",
        "who gives advice to people about living a good life.\\\n",
        "You attempt to provide unbiased advice.\n",
        "You respond in the tone of an Amitabh Bachchan.\n",
        "\"\"\"\n",
        "\n",
        "prompt = f\"\"\"\n",
        "{role}\n",
        "how to create a list and add an element to it.\n",
        "\"\"\"\n",
        "response = llama(prompt)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "limfSy2ITku-"
      },
      "source": [
        "#### Change the role to `friendly coding mentor` and see how the response changes for the same task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3FpEZmfITku-"
      },
      "outputs": [],
      "source": [
        "# your code here\n",
        "role = \"\"\"\n",
        "Your role is a friendly coding mentor\n",
        "who gives advice to people learning to code.\n",
        "You provide clear, beginner-friendly explanations.\n",
        "You respond in a warm, encouraging tone.\n",
        "\"\"\"\n",
        "prompt = f\"\"\"\n",
        "{role}\n",
        "how to create a list and add an element to it.\n",
        "\"\"\"\n",
        "response = llama(prompt)\n",
        "print(response)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rUfjq56Tku-"
      },
      "source": [
        "## Asking follow-up questions\n",
        "\n",
        "### Does the model have memory of the previous conversation?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y-XQCIVsTku-"
      },
      "outputs": [],
      "source": [
        "prompt_1 = \"\"\"\n",
        "What are fun activities I can do this weekend?\n",
        "\"\"\"\n",
        "\n",
        "response_1 = llama(prompt_1)\n",
        "print(response_1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oPR5leRcTku_"
      },
      "outputs": [],
      "source": [
        "prompt_2 = \"\"\"\n",
        "Which of these would be good for my health?\n",
        "\"\"\"\n",
        "response_2 = llama(prompt_2)\n",
        "print(response_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3qOB8QdTku_"
      },
      "source": [
        "#### Is the the second answer related to the first answer?\n",
        "#### **Note:** LLMs are `stateless` models, so they don't have memory of the previous conversation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGHE7OzBTku_"
      },
      "source": [
        "## Multi-turn prompting (chatting)\n",
        "#### In order to give the model memory of the previous conversation, you need to provide prior prompts and responses as part of the context of each new turn in the conversation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TXKGAF6dTku_"
      },
      "outputs": [],
      "source": [
        "image_path = './assets/multi_turn.png'\n",
        "display(Image(image_path, width=600))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nEAyQcBJTku_"
      },
      "source": [
        "### Note: you donit need `end tag (</s>)` for the last prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2qjMQfTtTku_"
      },
      "outputs": [],
      "source": [
        "chat_prompt = f\"\"\"\n",
        "<s>[INST] {prompt_1} [/INST]\n",
        "{response_1}\n",
        "</s>\n",
        "<s>[INST] {prompt_2} [/INST]\n",
        "\"\"\"\n",
        "print(chat_prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XnAxWREPTku_"
      },
      "source": [
        "### Note: pay attention to add_inst (add instruction) argument below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tNzt7OueTkvF"
      },
      "outputs": [],
      "source": [
        "response_2 = llama(chat_prompt,\n",
        "                 add_inst=False,\n",
        "                 verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h68xn5bnTkvF"
      },
      "outputs": [],
      "source": [
        "print(response_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1rnXleQTkvF"
      },
      "source": [
        "### Helper function to handle multi-turn prompting\n",
        "\n",
        "### **Note:** You don’t need to understand every part of the helper function. In the next section, you’ll see how to use it in your code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v-fpsRdygQGr"
      },
      "outputs": [],
      "source": [
        "def llama_chat(prompts,\n",
        "               responses,\n",
        "               model=\"meta-llama/Llama-2-7b-chat-hf\",\n",
        "               temperature=0.0,\n",
        "               max_tokens=1024,\n",
        "               verbose=False,\n",
        "               url=url,\n",
        "               headers=headers,\n",
        "               base=2,\n",
        "               max_tries=3\n",
        "              ):\n",
        "\n",
        "    prompt = get_prompt_chat(prompts,responses)\n",
        "\n",
        "    # Allow multiple attempts to call the API incase of downtime.\n",
        "    # Return provided response to user after 3 failed attempts.\n",
        "    wait_seconds = [base**i for i in range(max_tries)]\n",
        "\n",
        "    for num_tries in range(max_tries):\n",
        "        try:\n",
        "            response = llama(prompt=prompt,\n",
        "                             add_inst=False,\n",
        "                             model=model,\n",
        "                             temperature=temperature,\n",
        "                             max_tokens=max_tokens,\n",
        "                             verbose=verbose,\n",
        "                             url=url,\n",
        "                             headers=headers\n",
        "                            )\n",
        "            return response\n",
        "        except Exception as e:\n",
        "            if response.status_code != 500:\n",
        "                return response.json()\n",
        "\n",
        "            print(f\"error message: {e}\")\n",
        "            print(f\"response object: {response}\")\n",
        "            print(f\"num_tries {num_tries}\")\n",
        "            print(f\"Waiting {wait_seconds[num_tries]} seconds before automatically trying again.\")\n",
        "            time.sleep(wait_seconds[num_tries])\n",
        "\n",
        "    print(f\"Tried {max_tries} times to make API call to get a valid response object\")\n",
        "    print(\"Returning provided response\")\n",
        "    return response\n",
        "\n",
        "\n",
        "def get_prompt_chat(prompts, responses):\n",
        "  prompt_chat = f\"<s>[INST] {prompts[0]} [/INST]\"\n",
        "  for n, response in enumerate(responses):\n",
        "    prompt = prompts[n + 1]\n",
        "    prompt_chat += f\"\\n{response}\\n </s><s>[INST] \\n{ prompt }\\n [/INST]\"\n",
        "\n",
        "  return prompt_chat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zC0gE4EETkvG"
      },
      "source": [
        "### How to use the helper function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "laPRgntYTkvG"
      },
      "outputs": [],
      "source": [
        "prompt_1 = \"\"\"\n",
        "    What are fun activities I can do this weekend?\n",
        "\"\"\"\n",
        "\n",
        "response_1 = llama(prompt_1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PqZGqej2TkvG"
      },
      "outputs": [],
      "source": [
        "prompt_2 = \"\"\"\n",
        "Which of these would be good for my health?\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Owh02eUjTkvG"
      },
      "outputs": [],
      "source": [
        "prompts = [prompt_1,prompt_2]\n",
        "responses = [response_1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y3g-pXDOTkvG"
      },
      "outputs": [],
      "source": [
        "prompts = [prompt_1,prompt_2]\n",
        "responses = [response_1]\n",
        "\n",
        "# Pass prompts and responses to llama_chat function\n",
        "response_2 = llama_chat(prompts,responses,verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XRyRAi7dTkvG"
      },
      "outputs": [],
      "source": [
        "print(response_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZF11qERTkvG"
      },
      "source": [
        "### Excercise 3: Multi-turn prompting\n",
        "\n",
        "### Ask this follow-up question: \"Which of these activites would be fun with friends?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dB56VJpPTkvG"
      },
      "outputs": [],
      "source": [
        "prompt_3 = \"Which of these activites would be fun with friends?\"\n",
        "\n",
        "#your code here\n",
        "role = \"\"\"\n",
        "Your role is a friendly coding mentor \\\n",
        "who gives advice to people learning to code.\\\n",
        "You provide clear, beginner-friendly explanations.\n",
        "You respond in a warm, encouraging tone.\n",
        "\"\"\"\n",
        "\n",
        "prompt_3 = \"Which of these activities would be fun with friends?\"\n",
        "\n",
        "full_prompt = f\"\"\"\n",
        "{role}\n",
        "{prompt_3}\n",
        "\"\"\"\n",
        "\n",
        "response = llama(full_prompt)\n",
        "print(response)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gAqvSPYnTkvG"
      },
      "source": [
        "### OrderBot\n",
        "\n",
        "#### We can `automate the collection of user prompts and model responses` to build a  OrderBot.\n",
        "\n",
        "#### The OrderBot will take orders at a pizza restaurant."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SgVECoD8WH0U"
      },
      "outputs": [],
      "source": [
        "# Define the bot's role and menu\n",
        "role = \"\"\"\n",
        "You are OrderBot, an automated service to collect orders for a pizza restaurant. \\\n",
        "You first greet the customer, then start collecting the order, \\\n",
        "and then asks if it's a pickup or delivery. \\\n",
        "You wait to collect the entire order, then summarize it and check for a final \\\n",
        "time if the customer wants to add anything else. \\\n",
        "If it's a delivery, you ask for an address. \\\n",
        "Finally you collect the payment.\\\n",
        "Make sure to clarify all options, extras and sizes to uniquely \\\n",
        "identify the item from the menu.\\\n",
        "You respond in a short, very conversational friendly style. \\\n",
        "The menu includes \\\n",
        "pepperoni pizza  12.95, 10.00, 7.00 \\\n",
        "cheese pizza   10.95, 9.25, 6.50 \\\n",
        "eggplant pizza   11.95, 9.75, 6.75 \\\n",
        "fries 4.50, 3.50 \\\n",
        "greek salad 7.25 \\\n",
        "Toppings: \\\n",
        "extra cheese 2.00, \\\n",
        "mushrooms 1.50 \\\n",
        "sausage 3.00 \\\n",
        "canadian bacon 3.50 \\\n",
        "AI sauce 1.50 \\\n",
        "peppers 1.00 \\\n",
        "Drinks: \\\n",
        "coke 3.00, 2.00, 1.00 \\\n",
        "sprite 3.00, 2.00, 1.00 \\\n",
        "bottled water 5.00 \\\n",
        "\"\"\" # accumulate messages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqa-1yg7WH0U"
      },
      "source": [
        "## Excercise 4: Orderbot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FbvuUb_yWH0U"
      },
      "outputs": [],
      "source": [
        "prompts = []\n",
        "responses = []\n",
        "\n",
        "prompts.append(role)\n",
        "response = llama_chat(prompts, responses)\n",
        "responses.append(response)\n",
        "print(f\"\\nChatbot: {response}\\n\")\n",
        "\n",
        "while True:\n",
        "\n",
        "    # your code here\n",
        "    prompt = ...\n",
        "    prompts.append(prompt)\n",
        "    response = ...\n",
        "    responses.append(response)\n",
        "    print(f\"\\nChatbot: {response}\\n\")\n",
        "    if prompt == \"exit\" or \"done\" or \"quit\" or \"bye\":\n",
        "        break\n",
        "\n",
        "    # get the input from the user with input() function\n",
        "    # change prompts and responses accordingly\n",
        "    # terminate the loop if the user types 'done', 'exit', 'quit', or bye\n",
        "    ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Z_SMMmmWH0U"
      },
      "source": [
        "### Printing the order"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S7P-7fJsTkvH"
      },
      "outputs": [],
      "source": [
        "role = 'create a json summary of the food order. Itemize the price for each item\\\n",
        " The fields should be 1) pizza, include size 2) list of toppings 3) list of drinks, include size\\\n",
        "          4) list of sides include size  5)total price '\n",
        "\n",
        "messages = prompts.copy()\n",
        "messages.append(role)\n",
        "\n",
        "response = llama_chat(messages, responses)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APImWn5LTkvH"
      },
      "source": [
        "## Excercise 5: Improving the OrderBot\n",
        "\n",
        "### Try to improve the performance of your chat by\n",
        "\n",
        "### 1. modifying the prompt\n",
        "### 2. using different models (larger ones)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pYUYSe0eTkvH"
      },
      "outputs": [],
      "source": [
        "role = \"\"\"\n",
        "You are OrderBot, an AI assistant for a pizza restaurant. Follow these steps:\n",
        "\n",
        "1. Greet the customer warmly.\n",
        "2. Present the menu clearly, including sizes and prices.\n",
        "3. Take the order, asking for specifics (size, toppings, etc.) one item at a time.\n",
        "4. Confirm each item before moving to the next.\n",
        "5. Ask if it's for pickup or delivery.\n",
        "6. If delivery, request the address.\n",
        "7. Summarize the entire order and total cost.\n",
        "8. Ask if they want to add anything else.\n",
        "9. Collect payment information.\n",
        "10. Thank the customer and provide an estimated ready/delivery time.\n",
        "\n",
        "Respond concisely and conversationally. Be friendly and helpful.\n",
        "\n",
        "Menu:\n",
        "Pizzas (Large/Medium/Small):\n",
        "- Pepperoni: $12.95/$10.00/$7.00\n",
        "- Cheese: $10.95/$9.25/$6.50\n",
        "- Eggplant: $11.95/$9.75/$6.75\n",
        "\n",
        "Sides:\n",
        "- Fries: $4.50 (Large), $3.50 (Small)\n",
        "- Greek Salad: $7.25\n",
        "\n",
        "Toppings:\n",
        "- Extra Cheese: $2.00\n",
        "- Mushrooms: $1.50\n",
        "- Sausage: $3.00\n",
        "- Canadian Bacon: $3.50\n",
        "- AI Sauce: $1.50\n",
        "- Peppers: $1.00\n",
        "\n",
        "Drinks (Large/Medium/Small):\n",
        "- Coke: $3.00/$2.00/$1.00\n",
        "- Sprite: $3.00/$2.00/$1.00\n",
        "- Bottled Water: $5.00\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "def llama_chat (prompts, responses):\n",
        "    messages = [{\"role\": \"system\", \"content\": prompts[0]}]\n",
        "    for prompt, response in zip(prompts[1:], responses):\n",
        "        messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "        messages.append({\"role\": \"assistant\", \"content\": response})\n",
        "\n",
        "    response = openai.ChatCompletion.create(\n",
        "      model=\"llama_chat\",\n",
        "      messages=messages\n",
        "    )\n",
        "    return response.choices[0].message['content']\n"
      ],
      "metadata": {
        "id": "jVKzS5u_Qg8O"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}